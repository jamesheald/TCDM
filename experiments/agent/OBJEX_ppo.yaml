name: OBJEX_PPO                                             # algorithm name
multi_proc: True                                            # whether to use multi_processing or not
params:
  gamma: 0.95                                               # discount factor
  gae_lambda: 0.95                                          # GAE parameter
  learning_rate: 1e-5                                       # learning rate for optimizer
  ent_coef: 0.001                                           # PPO Entropy coefficient
  vf_coef: 0.5                                              # Added multiplier on Value Function Loss
  clip_range: 0.2                                           # PPO Clip parameter
  # n_steps: 4096                                             # Number of timesteps to collect per iteration (e.g. global batch) 
  n_steps: 4080                                             # Number of timesteps to collect per iteration (e.g. global batch) 
  # batch_size: 256                                           # Minibatch size for each ADAM step during PPO opt
  batch_size: 255                                           # Minibatch size for each ADAM step during PPO opt
  n_epochs: 5                                               # Number of epochs on collected global batch per iteration
  warm_start_mean: True                                     # Warm start mean to explore around initial position
  # guide_coef: 1000000
  guide_coef: 0.
  dynamics_learning_rate: 1e-5
  dynamics_n_epochs: 5
  net_arch_dyn: [256, 128]
  dynamics_dropout: [0., 0.]
  state_dependent_std:
    diagonal: True
    low_rank: True
  target_entropy:
    diagonal: null # -30 or null
    explore: -7
  ent_coef_lr: 1e-5
  clip_grad_ent_coef: True
  use_tanh_bijector: False
  diagonal_entropy_when_touching: True
  explore_ent_coef: 0.001
  off_manifold_std: 
    state_dependent: False
    action_dependent: False
  use_gram_schmidt: True
policy_kwargs:                                              # Policy parameters (initial STD and architecture)
  net_arch:
    - pi: [256, 128]
      vf: [256, 128]
      ex: [256, 128]
  log_std_init: -1.60

controlled_variables: ObjCvelForce # ObjQvelForce ObjQvelForceTable pincer ObjQvel ObjCvelForce

standard_PPO: False

std_network: one_policy_network

single_entropy: False

jax_enable_x64: False