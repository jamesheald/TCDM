name: OBJEX_PPO                                             # algorithm name
multi_proc: True                                            # whether to use multi_processing or not
params:
  gamma: 0.95                                               # discount factor
  gae_lambda: 0.95                                          # GAE parameter
  learning_rate: 1e-5                                       # learning rate for optimizer
  ent_coef: 0.001                                           # PPO Entropy coefficient
  vf_coef: 0.5                                              # Added multiplier on Value Function Loss
  clip_range: 0.2                                           # PPO Clip parameter
  # n_steps: 4096                                             # Number of timesteps to collect per iteration (e.g. global batch) 
  n_steps: 4080                                             # Number of timesteps to collect per iteration (e.g. global batch) 
  # batch_size: 256                                           # Minibatch size for each ADAM step during PPO opt
  batch_size: 255                                           # Minibatch size for each ADAM step during PPO opt
  n_epochs: 5                                               # Number of epochs on collected global batch per iteration
  warm_start_mean: True                                     # Warm start mean to explore around initial position
  # guide_coef: 1000000
  guide_coef: 0.
  dynamics_learning_rate: 1e-5
  dynamics_n_epochs: 40
  net_arch_dyn: [400, 300]
  # net_arch_dyn: [256, 256]
  dynamics_dropout: [0., 0.]
  state_dependent_std:
    diagonal: True
    low_rank: True
  target_entropy: -30. # null -30
  # target_entropy:
  #   diagonal: null # -30 or null
  #   explore: -7
  ent_coef_lr: 1e-5
  clip_grad_ent_coef: True
  use_tanh_bijector: False
  # diagonal_entropy_when_touching: True
  diagonal_entropy: not_touch_hand # always not_touch_hand touch_table
  explore_ent_coef: 0.001
  off_manifold_std: 
    state_dependent: False
    action_dependent: False
  use_gram_schmidt: True
policy_kwargs:                                              # Policy parameters (initial STD and architecture)
  net_arch:
    # - pi: [256, 256]
      # vf: [256, 256]
      # ex: [256, 256]
    - pi: [400, 300]
      vf: [400, 300]
      ex: [400, 300]
  log_std_init: -1.60
  zlog_std_init: -1.60 # -1.6, -1.9, -2.3, -2.6, -3, -3.5

controlled_variables: ObjCvelForce # ObjQvelForce ObjQvelForceTable pincer ObjQvel ObjCvelForce ObjCvelForceTable

standard_PPO: False

std_network: one_policy_network

single_entropy: True

jax_enable_x64: False